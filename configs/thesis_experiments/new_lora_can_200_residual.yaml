algorithm:
  anneal_learning_rate: false
  batch_size: 256
  nr_hidden_units: 1024
  bottleneck_size: 48
  buffer_size: 1000000
  concat_joint_state: true
  concat_object_state: false
  critic_and_reward_utd: 1
  critic_grad_norm_scaling: 1
  critic_learning_rate: 0.0003
  critic_reset_interval: -1
  critic_warmup_steps: 0
  damping: 1
  embedding_dim: 1024
  demo_to_online_ratio: 0.1
  embedding_noise_alpha: 250
  entropy_coefficient: 0.1
  evaluation_episodes: 25
  evaluation_frequency: 50000
  final_evaluation_episodes: 50
  gamma: 0.99
  image_based_csil: false
  is_normal_env: false
  is_robomimic_env: true
  is_simpler_env: false
  joint_space_obs: false
  layer_norm_policy: true
  learning_starts: 10000
  load_tfds_data: false
  max_reward: 6.215108223463887
  min_var: 1.0e-05
  negative_reward: true
  normalize_actions: true
  normalize_observations: false
  num_recordings_per_eval: 5
  plot_actions: false
  policy_learning_rate: 0.0003
  record_before_after: false
  record_eval_episodes: true
  refine_reward: true
  residual_action_scaling: null
  reward_grad_norm_clip_threshold: -1
  reward_kl_scale_factor: 1
  reward_learning_rate: 0.001
  reward_refinement_steps: -1
  reward_scale_factor: 0.16
  save_training_recordings: false
  scale_log_ratios_by_alpha: true
  start_with_evaluation: true
  stationary_activation_function_str: per_relu
  tau: 0.005
  total_timesteps: 1000000
  training_recording_frequency: 20
  use_embedding_noise: false
  use_embedding_noise_in_rl: false
  use_ibrl: false
  use_linear_residual_combination: false
  use_vla: true
  use_vla_action_for_hetstat: true
  action_normalization_str: linear
  freeze_embeddings: false
checkpoints:
  checkpoint_dir: null
  max_num_checkpoints: 10
  reload_checkpoint: false
  saving_frequency: -1
environment:
  max_episode_length: 400
  name: PickPlaceCan
  nr_envs: 1
  render_fps: 30
  task_prompt: put can in field with can
general:
  start_wandb_run: true
  wandb_project_type: debug
  experiment_name: 'new lora model - residual'
  root_dir_path: /home/scherer/csil
  save_replay_buffers: false
  seed: 7
  use_double_precision: false
  algorithm: csil
pretraining:
  critic_learning_rate: 0.001
  critic_pretrain_steps: 2000
  dataset_filename: /home/scherer/csil/robomimic_datasets/can/ph/high_dim_v141.hdf5
  entropy_coefficient_init: 0
  eval_frequency: -1
  num_demonstrations: 200
  policy_learning_rate: 0.0001
  policy_pretrain_steps: 5000
  replay_buffer_evaluation_episodes: 20
  save_checkpoint: false
  target_entropy_factor: 5
  tau: 0.005
  use_parameter_kl_term: false
  vla_noise_stddev: 0.008
  stochastic_eval: false
  evaluation_at_steps: []
vla:
  pizero_config_name: pi0_mimicgen_robomimic_ph
  checkpoint_dir: /home/scherer/csil/vla_checkpoints/pi0_mimicgen_robomimic_ph/balanced_mimicgen_lora/29999
  vla_type: balanced_ds_general_4_tasks
